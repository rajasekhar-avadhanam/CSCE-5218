{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "8528e6cb-701a-4c85-caad-52f2a1753a04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',\n",
              " '                                 Dload  Upload   Total   Spent    Left  Speed',\n",
              " '',\n",
              " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
              " ' 73 11645   73  8523    0     0  13364      0 --:--:-- --:--:-- --:--:-- 13358',\n",
              " '100 11645  100 11645    0     0  17739      0 --:--:-- --:--:-- --:--:-- 17724']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!!/usr/bin/curl --output test.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/test_small.txt\n",
        "!!/usr/bin/curl --output train.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/train.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "1a0b7b5b-c803-46f8-9b8f-f3bea4be0469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\r\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\r\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\r\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\r\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\r\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\r\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\r\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "X1\tX2\tX3\n",
            "1\t1\t1\t1\n",
            "0\t0\t1\t1\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "1\t1\t1\t1\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    #TODO: Return dot product of array 1 and array 2\n",
        "    return sum(a * b for a, b in zip(array1, array2))\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    #TODO: Return outpout of sigmoid function on x\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# The output of the model, which for the perceptron is\n",
        "# the sigmoid function applied to the dot product of\n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    #TODO: return the output of the model\n",
        "    return sigmoid(dot_product(weights, instance))\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    #TODO: return the prediction of the model\n",
        "    return 1 if output(weights, instance) >= 0.5 else 0\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate)\n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "    # Weights Initialization to 0.\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            #Foward Pass Through Netwrok to compute Weights\n",
        "            in_value = dot_product(weights, instance)\n",
        "            output = sigmoid(in_value)\n",
        "            error = instance[-1] - output\n",
        "            #Updating weights from Backpropagation.\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8bc717-dec1-46a8-ce86-f6afe801d32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "The perceptron model uses the Gradient Descent to update the weights and it requires to be differtiable. sigmoid ensure that the output is differentiable.\n",
        "\n",
        "Instaed of Sigmoid if we use predict() it gives us the values of 1 or 0 which are discrete and results in loss of gradient info for weight updates. so we use sigmoid activation function instead of predict.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above)\n",
        "\n",
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fca4b1-5155-4616-c52e-ed3c46c2495f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
            "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
            "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
            "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n",
        "\n",
        "A: from the results we ca see that we got highest accuracy with 5% dataset with lr of 0.005  we can also see other instaces where increasing training data helps improve the accuracy but training data size is not the only factor we have to tune the repective hyper parameters which aids large traing datasets to improve accuracy.\n",
        "\n",
        "\n",
        "B: From the two runs we can see that training with 200 instances with 20 epochs and lr 0f 0.005 has lower accuracy compared to 100 instances with 20 epochs and lr of 0.05. This may be because of the lreaning rate beaing too small for second run which is 0.005 which can take longer to converge the optimal solution when compared to first run with lr of 0.05. So the parameter that affeted the result could be the learning rate in these two runs.\n",
        "\n",
        "C: With the parameters used, we managed to get the accuracy of above 80%, adding some additional paramaeters will be helpful like batch size but we have to make sure to fine tune the model with all the parameters that helps other to imporve the accuracy.\n",
        "\n",
        "\n",
        "\n",
        "D: Not really, increasing the epochs doent always increase the accuracy. We can see some instances where the epochs are 100 but still we get the least accuracy of 28.6% this is because the model is overfitted by just memorizing the traing data. We also have another istance with epoch=100 but the accuracy is 42.9%. Frome these we can infer with more epochs we dont get higher accuracy always. we have to make other parameters complimenting the epoch to get higher accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}